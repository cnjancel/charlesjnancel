)
circle_stats <- list(
rows_columns = dim(circle),
mean_x = mean(circle$x),
mean_y = mean(circle$y)
)
wide_lines_stats <- list(
rows_columns = dim(wide_lines),
mean_x = mean(wide_lines$x),
mean_y = mean(wide_lines$y)
)
list(dino = dino_stats, dots = dots_stats, circle = circle_stats, wide_lines = wide_lines_stats)
# For the dino dataset
dino_cor <- cor(dino$x, dino$y)
dino_lm <- lm(y ~ x, data = dino)
dino_coeffs <- coef(dino_lm)
# For the dots dataset
dots_cor <- cor(dots$x, dots$y)
dots_lm <- lm(y ~ x, data = dots)
dots_coeffs <- coef(dots_lm)
# For the circle dataset
circle_cor <- cor(circle$x, circle$y)
circle_lm <- lm(y ~ x, data = circle)
circle_coeffs <- coef(circle_lm)
# For the wide_lines dataset
wide_lines_cor <- cor(wide_lines$x, wide_lines$y)
wide_lines_lm <- lm(y ~ x, data = wide_lines)
wide_lines_coeffs <- coef(wide_lines_lm)
# Print out the results
list(
dino = list(correlation = dino_cor, coefficients = dino_coeffs),
dots = list(correlation = dots_cor, coefficients = dots_coeffs),
circle = list(correlation = circle_cor, coefficients = circle_coeffs),
wide_lines = list(correlation = wide_lines_cor, coefficients = wide_lines_coeffs)
)
# Dino plot
ggplot(dino, aes(x, y)) +
geom_point() +
labs(title = "dino", x = "x", y = "y")
# Dots plot
ggplot(dots, aes(x, y)) +
geom_point() +
labs(title = "dots", x = "x", y = "y")
# Circle plot
ggplot(circle, aes(x, y)) +
geom_point() +
labs(title = "circle", x = "x", y = "y")
# Wide_lines plot
ggplot(wide_lines, aes(x, y)) +
geom_point() +
labs(title = "wide_lines", x = "x", y = "y")
?birthwt
library(MASS)
# Getting the number of observations
num_observations <- nrow(birthwt)
# Getting the number of variables
num_variables <- ncol(birthwt)
num_observations
num_variables
# Assuming you have loaded the MASS package and its dataset
library(MASS)
# Selecting the relevant columns
selected_data <- birthwt[, c("bwt", "age", "lwt")]
# Creating the correlation matrix
cor_matrix <- cor(selected_data)
cor_matrix
library(ggplot2)
# Scatter plot for lwt vs bwt
ggplot(birthwt, aes(x = lwt, y = bwt)) +
geom_point() +
labs(title = "Relationship between Mother's Pre-pregnancy Weight and Baby's Weight",
x = "Mother's Pre-pregnancy Weight (lwt)",
y = "Baby's Weight (bwt)") +
theme_minimal()
# Fitting the linear model
model <- lm(bwt ~ lwt, data = birthwt)
# Displaying the model summary to retrieve coefficients
summary(model)
# Given coefficients and lwt value
intercept <- 2369.624
slope <- 4.429
lwt_value <- 147
# Calculate predicted bwt
predicted_bwt <- intercept + slope * lwt_value
# Calculate the residual
observed_bwt <- 3000
residual <- observed_bwt - predicted_bwt
predicted_bwt
residual
observed_bwt = 2743
residual_value = -40
# Calculate lwt using the relationship
lwt_original <- (observed_bwt - intercept - residual_value) / slope
lwt_original
library(ggplot2)
?msleep
sleep_model <- lm(sleep_total ~ bodywt, data=msleep)
summary(sleep_model)
r_squared <- summary(sleep_model)$r.squared
r_squared
r <- sqrt(r_squared)
if (coef(sleep_model)["bodywt"] < 0) {
r <- -r
}
r
x_bar <- mean(msleep$bodywt, na.rm = TRUE)
y_bar <- mean(msleep$sleep_total, na.rm = TRUE)
S_xx <- sum((msleep$bodywt - x_bar)^2, na.rm = TRUE)
S_xy <- sum((msleep$bodywt - x_bar) * (msleep$sleep_total - y_bar), na.rm = TRUE)
x_bar
y_bar
S_xx
S_xy
beta_1 <- S_xy / S_xx
beta_0 <- y_bar - beta_1 * x_bar
beta_0
beta_1
y_hat <- beta_0 + beta_1 * msleep$bodywt
residuals <- msleep$sleep_total - y_hat
head(residuals)
SSR <- sum((y_hat - y_bar)^2)
SSE <- sum(residuals^2)
SST <- sum((msleep$sleep_total - y_bar)^2)
SSR
SSE
SST
R_squared_calculated <- SSR / SST
R_squared_calculated
chinchilla_bodywt <- 0.420
predicted_sleep <- beta_0 + beta_1 * chinchilla_bodywt
predicted_sleep
chinchilla_observed_sleep <- msleep$sleep_total[msleep$name == "Chinchilla"]
chinchilla_residual <- chinchilla_observed_sleep - predicted_sleep
chinchilla_residual
std_error_slope <- summary(sleep_model)$coefficients["bodywt", "Std. Error"]
alpha <- 0.2
df <- nrow(msleep) - 2
t_critical <- qt(1 - alpha/2, df)
margin_error <- t_critical * std_error_slope
lower_bound <- beta_1 - margin_error
upper_bound <- beta_1 + margin_error
c(lower_bound, upper_bound)
t_statistic <- (beta_1 - (-0.01)) / std_error_slope
p_value <- 2 * (1 - pt(abs(t_statistic), df))
c(t_statistic, p_value)
par(mfrow=c(2,2))
plot(sleep_model$fitted.values, residuals(sleep_model),
main="Residuals vs. Fitted", xlab="Fitted values", ylab="Residuals")
abline(h=0, col="red")
qqnorm(residuals(sleep_model), main="Normal Q-Q Plot")
qqline(residuals(sleep_model))
library(lmtest)
library(stats)
bp_test <- bptest(sleep_model)
shapiro_test <- shapiro.test(residuals(sleep_model))
list(Breusch_Pagan_Test = bp_test, Shapiro_Wilk_Test = shapiro_test)
library(ggplot2)
library(MASS)
# Load the necessary library
library(readr)
prof_evals <- read_csv("Prof_Evals.csv", show_col_types = FALSE)
cat("Number of professors:", nrow(prof_evals), "\n")
cat("Number of variables recorded for each professor:", ncol(prof_evals), "\n")
model <- lm(score ~ bty_avg, data = prof_evals)
summary(model)
sigma_value <- summary(model)$sigma
cat("Estimated value of sigma:", sigma_value, "\n")
x_vals <- prof_evals$bty_avg      # Independent variable (bty_avg)
sigma <- summary(model)$sigma     # Residual standard error from the model
sample_size <- nrow(prof_evals)   # Number of observations
xbar <- mean(x_vals)
Sxx <- sum((x_vals - xbar) ^ 2)
beta_1_sd <- sigma / sqrt(Sxx)
cat("Calculated standard error for the slope:", beta_1_sd, "\n")
slope_se <- coef(summary(model))["bty_avg", "Std. Error"]
cat("Standard error for the slope:", slope_se, "\n")
beauty_ratings <- c(6.25, 9.5)
predicted_scores <- predict(model, newdata = data.frame(bty_avg = beauty_ratings), interval = "confidence", level = 0.99)
lower_bounds <- predicted_scores[, "lwr"]
upper_bounds <- predicted_scores[, "upr"]
intervals <- data.frame(Beauty_Rating = beauty_ratings, Lower_Bound = lower_bounds, Upper_Bound = upper_bounds)
intervals
predicted_score_7_75 <- predict(model, newdata = data.frame(bty_avg = 7.75), interval = "prediction", level = 0.99)
lower_bound_7_75 <- predicted_score_7_75[1, "lwr"]
upper_bound_7_75 <- predicted_score_7_75[1, "upr"]
c(Lower_Bound = lower_bound_7_75, Upper_Bound = upper_bound_7_75)
beta_0 <- coef(model)["(Intercept)"]
beta_1 <- coef(model)["bty_avg"]
y_hat <- (4.0425 + 4.1624) / 2
beauty_rating <- (y_hat - beta_0) / beta_1
beauty_rating
multi_model <- lm(score ~ bty_avg + age + cls_students + cls_perc_eval, data = prof_evals)
summary(multi_model)
model_summary <- summary(multi_model)
t_statistic_bty_avg <- model_summary$coefficients["bty_avg", "t value"]
p_value_bty_avg <- model_summary$coefficients["bty_avg", "Pr(>|t|)"]
c(Test_Statistic = t_statistic_bty_avg, P_Value = p_value_bty_avg)
bty_avg_Z <- 6.25
age_Z <- 52
cls_students_Z <- 61
cls_perc_eval_Z <- 0.83
score_Z <- coef(multi_model)["(Intercept)"] +
coef(multi_model)["bty_avg"] * bty_avg_Z +
coef(multi_model)["age"] * age_Z +
coef(multi_model)["cls_students"] * cls_students_Z +
coef(multi_model)["cls_perc_eval"] * cls_perc_eval_Z
bty_avg_A <- 9.5
age_A <- 34
cls_students_A <- 270
cls_perc_eval_A <- 0.96
score_A <- coef(multi_model)["(Intercept)"] +
coef(multi_model)["bty_avg"] * bty_avg_A +
coef(multi_model)["age"] * age_A +
coef(multi_model)["cls_students"] * cls_students_A +
coef(multi_model)["cls_perc_eval"] * cls_perc_eval_A
c(Professor_Z_Score = score_Z, Professor_A_Score = score_A)
residual_Z <- 4.6 - score_Z
residual_A <- 3.7 - score_A
c(Residual_Professor_Z = residual_Z, Residual_Professor_A = residual_A)
prof_A_data <- data.frame(bty_avg = bty_avg_A,
age = age_A,
cls_students = cls_students_A,
cls_perc_eval = cls_perc_eval_A)
conf_interval_A <- predict(multi_model, newdata = prof_A_data,
interval = "confidence", level = 0.85)
conf_interval_A
prof_Z_data <- data.frame(bty_avg = bty_avg_Z,
age = age_Z,
cls_students = cls_students_Z,
cls_perc_eval = cls_perc_eval_Z)
pred_interval_Z <- predict(multi_model, newdata = prof_Z_data,
interval = "prediction", level = 0.75)
pred_interval_Z
model_r2 <- summary(model)$r.squared
multi_model_r2 <- summary(multi_model)$r.squared
c(Model_R2 = model_r2, Multiple_Model_R2 = multi_model_r2)
residuals <- residuals(model)
SSE <- sum(residuals^2)
SST <- sum((prof_evals$score - mean(prof_evals$score))^2)
residuals_multi <- residuals(multi_model)
SSE_multi <- sum(residuals_multi^2)
SST_multi <- sum((prof_evals$score - mean(prof_evals$score))^2)  # SST remains the same for both models
c(SSE = SSE, SSE_Multi = SSE_multi, SST = SST_multi)
# For the simple model
num_obs <- nrow(prof_evals)
num_parameters <- length(coef(model))
df <- num_obs - num_parameters
# For the multiple regression model
num_parameters_multi <- length(coef(multi_model))
df_multi <- num_obs - num_parameters_multi
c(DF = df, DF_Multi = df_multi)
knitr::opts_chunk$set(echo = TRUE)
library(arrow)
parquet_files <- c(
'Data/cfb_schedules_2001.parquet',
'Data/cfb_schedules_2002.parquet',
'Data/cfb_schedules_2003.parquet',
'Data/cfb_schedules_2004.parquet',
'Data/cfb_schedules_2005.parquet',
'Data/cfb_schedules_2006.parquet',
'Data/cfb_schedules_2007.parquet',
'Data/cfb_schedules_2008.parquet',
'Data/cfb_schedules_2009.parquet',
'Data/cfb_schedules_2010.parquet',
'Data/cfb_schedules_2011.parquet',
'Data/cfb_schedules_2012.parquet',
'Data/cfb_schedules_2013.parquet',
'Data/cfb_schedules_2014.parquet',
'Data/cfb_schedules_2015.parquet',
'Data/cfb_schedules_2016.parquet',
'Data/cfb_schedules_2017.parquet',
'Data/cfb_schedules_2018.parquet',
'Data/cfb_schedules_2019.parquet',
'Data/cfb_schedules_2020.parquet',
'Data/cfb_schedules_2021.parquet',
'Data/cfb_schedules_2022.parquet'
)
# Read the first file as a reference
merged_df <- arrow::read_parquet(parquet_files[1])
# Iterate through the rest of the files and bind them row-wise
for (file in parquet_files[-1]) {
temp_df <- arrow::read_parquet(file)
# Add missing columns with NA values
for (col in setdiff(names(merged_df), names(temp_df))) {
temp_df[[col]] <- NA
}
# Bind rows and retain only the columns present in the reference dataframe
merged_df <- rbind(merged_df, temp_df[, names(merged_df)])
}
library(dplyr)
# Dropping columns with high missingness
merged_df <- dplyr::select(merged_df, -c(start_time_tbd, venue_id, venue))
# Drop 'notes' and 'highlights' columns if they exist
if("notes" %in% colnames(merged_df)) {
merged_df$notes <- NULL
}
if("highlights" %in% colnames(merged_df)) {
merged_df$highlights <- NULL
}
if("season_type" %in% colnames(merged_df)) {
merged_df$season_type <- NULL
}
# Impute missing values for 'conference_game' with the most common value (mode)
common_conference_game <- which.max(table(merged_df$conference_game, useNA = "no"))
merged_df$conference_game[is.na(merged_df$conference_game)] <- common_conference_game == 2
# Imputing missing values for 'home_points' and 'away_points' with their respective medians
merged_df$home_points[is.na(merged_df$home_points)] <- median(merged_df$home_points, na.rm = TRUE)
merged_df$away_points[is.na(merged_df$away_points)] <- median(merged_df$away_points, na.rm = TRUE)
median_attendance <- median(merged_df$attendance, na.rm = TRUE)
merged_df$attendance[is.na(merged_df$attendance)] <- median_attendance
merged_df$excitement_index <- as.numeric(merged_df$excitement_index)
merged_df$away_post_win_prob <- as.numeric(merged_df$away_post_win_prob)
merged_df$home_post_win_prob <- as.numeric(merged_df$home_post_win_prob)
# Impute missing values with median
merged_df$excitement_index[is.na(merged_df$excitement_index)] <- median(merged_df$excitement_index, na.rm = TRUE)
merged_df$away_post_win_prob[is.na(merged_df$away_post_win_prob)] <- median(merged_df$away_post_win_prob, na.rm = TRUE)
merged_df$home_post_win_prob[is.na(merged_df$home_post_win_prob)] <- median(merged_df$home_post_win_prob, na.rm = TRUE)
# Filter out rows with missing ELO ratings
merged_df <- merged_df %>%
filter(!is.na(home_pregame_elo) & !is.na(away_pregame_elo))
merged_df <- merged_df %>%
filter(!is.na(away_postgame_elo) & !is.na(home_postgame_elo))
# Drop rows where specific columns have NA values
merged_df <- merged_df %>% filter(!is.na(home_conference) & !is.na(away_conference))
# Check the dimensions of the dataset after handling missing data
dim(merged_df)
# Check the structure of the modified dataframe
str(merged_df)
# Count NA values in each column
na_counts <- sapply(merged_df, function(x) sum(is.na(x)))
# Print the counts
print(na_counts)
head(merged_df)
arrow::write_parquet(merged_df, "merged_data.parquet")
write.csv(merged_df, "merged_data.csv", row.names = FALSE)
cat("Columns:\n")
print(names(merged_df))
cat("\nSample Data:\n")
print(head(merged_df))
# Calculate home and away wins for each game
merged_df <- merged_df %>%
mutate(home_win = ifelse(home_points > away_points, 1, 0),
away_win = ifelse(away_points > home_points, 1, 0))
# Aggregate total wins for home and away teams separately
home_wins <- merged_df %>%
group_by(season, home_team) %>%
summarise(total_home_wins = sum(home_win), .groups = 'drop')
away_wins <- merged_df %>%
group_by(season, away_team) %>%
summarise(total_away_wins = sum(away_win), .groups = 'drop')
# Resolve many-to-many join issue by ensuring no duplicated rows before joining
home_wins <- home_wins %>% distinct(season, home_team, .keep_all = TRUE)
away_wins <- away_wins %>% distinct(season, away_team, .keep_all = TRUE)
# Join back to the original merged_df to add the total wins for home and away teams
# Here we are assuming that there should be a one-to-one relationship between merged_df
# and the aggregated wins data frames.
merged_df <- merged_df %>%
left_join(home_wins, by = c("season", "home_team")) %>%
left_join(away_wins, by = c("season", "away_team"))
# If you need a single total_wins column per row, you can decide how to handle the games
merged_df <- merged_df %>%
mutate(total_wins = coalesce(total_home_wins, 0) + coalesce(total_away_wins, 0))
# Note: This assumes that for every game, the team will be listed once as a home team and once as an away team.
# If this assumption does not hold true, the logic for creating total_wins needs to be adjusted.
# Identify columns with only one unique value
single_unique_value_cols <- sapply(merged_df, function(x) if(length(unique(x)) == 1) TRUE else FALSE)
# Print the names of columns with only one unique value
names(single_unique_value_cols[single_unique_value_cols == TRUE])
sapply(lapply(merged_df, unique), length)
merged_df$neutral_site <- as.factor(merged_df$neutral_site)
merged_df$conference_game <- as.factor(merged_df$conference_game)
library(ggplot2)
ggplot(merged_df, aes(x = total_wins)) +
geom_histogram(bins = 30, fill = "blue", color = "black") +
labs(title = "Histogram of Total Wins", x = "Total Wins", y = "Frequency")
summary(merged_df$total_wins)
sd(merged_df$total_wins, na.rm = TRUE)
library(GGally)
ggpairs(merged_df, columns = c("total_wins", "home_pregame_elo", "away_pregame_elo"))
levels(as.factor(merged_df$conference_game))
levels(as.factor(merged_df$home_conference))
lm_model <- lm(total_wins ~ home_pregame_elo + away_pregame_elo +
as.factor(conference_game) + attendance +
I(week^2),
data = merged_df)
summary(lm_model)
par(mfrow = c(2, 2))
plot(lm_model)
library(car)
vif(lm_model)
single_level_factors <- sapply(merged_df, function(x) if(is.factor(x) && length(levels(x)) == 1) levels(x))
single_level_factors <- single_level_factors[!sapply(single_level_factors, is.null)]
print(single_level_factors)
library(MASS)
# Removing 'season_type' from the model as it has only one level
lm_model_for_boxcox <- lm(total_wins ~ home_pregame_elo + away_pregame_elo +
attendance + I(week^2), data = merged_df)
boxcox_result <- boxcox(lm_model_for_boxcox, plotit = TRUE)
# Determine the optimal lambda value
lambda_optimal <- boxcox_result$x[which.max(boxcox_result$y)]
cat("Optimal lambda for transformation:", lambda_optimal, "\n")
# Apply the Box-Cox transformation with the optimal lambda
merged_df$transformed_total_wins <- ifelse(lambda_optimal == 0,
log(merged_df$total_wins),
(merged_df$total_wins^lambda_optimal - 1) / lambda_optimal)
# Re-fit the model with the transformed response variable
lm_transformed <- lm(transformed_total_wins ~ home_pregame_elo + away_pregame_elo +
attendance + I(week^2), ,
data = merged_df)
summary(lm_transformed)
# Create a new binary variable indicating if the home conference is ACC or SEC
merged_df$is_ACC_or_SEC <- as.factor(merged_df$home_conference %in% c("ACC", "SEC"))
# Now perform the t-test
t_test_result <- t.test(total_wins ~ is_ACC_or_SEC, data = merged_df)
# View the results
print(t_test_result)
library(boot)
# Define the function for k-fold cross-validation
cv_error <- function(data, number_of_folds) {
cv_results <- cv.glm(data, glm(total_wins ~ home_pregame_elo + away_pregame_elo +
as.factor(conference_game) + attendance + I(week^2),
data = data), K = number_of_folds)
return(cv_results$delta)
}
# Perform 10-fold cross-validation
cv_error_result <- cv_error(merged_df, 10)
# Output the cross-validation estimated prediction error
cat("10-fold CV estimated prediction error:", cv_error_result[1], "\n")
# Creating a linear model with an interaction term
lm_model_interaction <- lm(total_wins ~ home_pregame_elo * conference_game +
away_pregame_elo + attendance + I(week^2),
data = merged_df)
# Output the summary of the model
summary(lm_model_interaction)
AIC(lm_model, lm_model_interaction)
# Fit your final selected model
final_model <- lm(total_wins ~ home_pregame_elo * conference_game +
away_pregame_elo + attendance + I(week^2), data = merged_df)
# Display a summary of the final model
summary(final_model)
residuals <- residuals(final_model)
residual_std_dev <- sd(residuals)
cat("Residual Standard Error (Standard Deviation of Residuals):", residual_std_dev, "\n")
vif_results <- vif(final_model, type = "predictor")
cat("Variance Inflation Factors (VIF):\n")
print(vif_results)
# Residual plots to check model assumptions
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 grid
plot(final_model)
# Identify and plot outliers using Cook's distance
cooksd <- cooks.distance(final_model)
plot(cooksd, pch = "18", main = "Cook's Distance Plot")
abline(h = 4/length(cooksd), col = "red")
formula(final_model)
library(boot)
# Define the number of folds
k <- 10
# Define the statistic to be calculated at each fold
cv_statistic <- function(data, indices) {
indices <- as.integer(indices)
train <- merged_df[indices, ]
test <- merged_df[-indices, ]
# Fit the model to the training set
fit <- lm(final_model, data = train)
# Predict on the test set
predictions <- predict(fit, test)
# Calculate and return the MSE
mean((test$total_wins - predictions)^2)
}
# Perform k-fold cross-validation
cv_results <- cv.glm(data = merged_df, glmfit = final_model, K = k, cost = cv_statistic)
# Output the results
print(cv_results$delta)
# Convert conference_game to binary numeric variable
merged_df$conference_game_numeric <- as.numeric(merged_df$conference_game) - 1
# Subset data
conference_wins <- merged_df$total_wins[merged_df$conference_game_numeric == 1]
non_conference_wins <- merged_df$total_wins[merged_df$conference_game_numeric == 0]
# Perform the t-test
t_test_results <- t.test(conference_wins, non_conference_wins)
# Print the results
print(t_test_results)
# tinytex::install_tinytex()
# install.packages('ggplot2')
(14-5)^(4+7)* (8/23)
pi^(7.6)
z=seq(4,11,1)
y = pi^z - pi^(7.6)
print(y)
print(sqrt(sum(y^2)))
sb = as.data.frame(Seatbelts)
sb$law = as.factor(sb$law)
dim(sb)
nrow(sb)
ncol(sb)
head(sb, 6)
summary(sb$front)
sd(sb$front)
summary(sb$rear)
sd(sb$rear)
library(ggplot2)
hist(sb$front)
hist(sb$rear)
ggplot(data = sb, mapping = aes(y = front, x = rear)) +
geom_point()
ggplot(data = sb, mapping = aes(y = front, x = rear, shape =
law, color = law)) + geom_point() + geom_smooth(method = 'lm', se =
F, formula = y ~ x)
sum(1:10)
sum(11:20)
(1:10)^2
ggplot(data = pressure, mapping = aes(x = temperature, y = pressure)) + geom_point() + scale_y_log10() + geom_smooth(method = 'lm', se = F, formula = 'y ~ x')
